---
title: "Introduction to dbplyr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to dbplyr}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
options(tibble.print_min = 4L, tibble.print_max = 4L)
```

As well as working with local in-memory data like data frames and data tables, dplyr also works with remote on-disk data stored in databases. Generally, if your data fits in memory there is no advantage to putting it in a database: it will only be slower and more hassle. The reason you'd want to use dplyr with a database is because either your data is already in a database (and you don't want to work with static csv files that someone else has dumped out for you), or you have so much data that it does not fit in memory and you have to use a database. Currently dplyr supports the three most popular open source databases (sqlite, mysql and postgresql), and google's bigquery.

Since R almost exclusively works with in-memory data, if you do have a lot of data in a database, you can't just dump it into R. Instead, you'll have to work with subsets or aggregates. dplyr aims to make this task as easy as possible. If you're working with large data, it's also likely that you'll need support to get the data into the database and to ensure you have the right indices for good performance. While dplyr provides some simple tools to help with these tasks, they are no substitute for a local expert.

The motivation for supporting databases in dplyr is that you never pull down the right subset or aggregate from the database on your first try. Usually you have to iterate between R and SQL many times before you get the perfect dataset. But because switching between languages is cognitively challenging (especially because R and SQL are so perilously similar), dplyr helps you by allowing you to write R code that is automatically translated to SQL. The goal of dplyr is not to replace every SQL function with an R function; that would be difficult and error prone. Instead, dplyr only generates `SELECT` statements, the SQL you write most often as an analyst.

To get the most out of this chapter, you'll need to be familiar with querying SQL databases using the `SELECT` statement. If you have some familiarity with SQL and you'd like to learn more, I found [how indexes work in SQLite](http://www.sqlite.org/queryplanner.html) and [10 easy steps to a complete understanding of SQL](http://blog.jooq.org/2016/03/17/10-easy-steps-to-a-complete-understanding-of-sql) to be particularly helpful.

## Getting started

The easiest way to experiement with databases using dplyr is to use SQLite. This is because everything you need is already included in the R package. You won't need to install anything, and you won't need to deal with the hassle of setting up a database server. 

To work with a remote database in dplyr, you first make a connection using `DBI::dbConnect()`. For SQLite this takes two arguments: the database driver, and the path to the database: 

```{r, message = FALSE}
library(dplyr)
con <- DBI::dbConnect(RSQLite::SQLite(), path = ":memory:")
```

(Here we use the special string ":memory:" which causes SQLite to make a temporary in-memory database)

SQLite is easy to use because it runs the server for you on your own computer. Most existing databases live somewhere else, so if you're connecting to your company's database, the code will look more like:

```{r, eval = FALSE}
con <- DBI::dbConnect(RMySQL::MySQL(), 
  host = "database.rstudio.com",
  user = "hadley",
  password = rstudioapi::askForPassword()
)
```

This database has no data in it, so we'll start by copying over `nycflights13::flights` using the convenient `copy_to()` function. This is a quick and dirty way of getting data into a database. Because all the data has to flow through R, you should note that this is not suitable for very large datasets.

```{r}
copy_to(con, nycflights13::flights, "flights",
  temporary = FALSE, 
  indexes = list(
    c("year", "month", "day"), 
    "carrier", 
    "tailnum"
  )
)
```

As you can see, the `copy_to()` operation has an additional argument that allows you to supply indexes for the table. Here we set up indexes that will allow us to quickly process the data by day, by carrier and by plane. `copy_to()` also executes the SQL `ANALYZE` command: this ensures that the database has up-to-date table statistics and performs the appropriate query optimisations.

Now that we've copied the data, we can use `tbl()` to take a reference to it:

```{r}
flights_db <- tbl(con, "flights")
```

When you print it out, you'll notice that it looks like a regular tibble:

```{r}
flights_db 
```

## Basic verbs

Whether you're dealing with remote or local data sources, you use the same five verbs:

```{r}
flights_db %>% select(year:day, dep_delay, arr_delay)
flights_db %>% filter(dep_delay > 240)
flights_db %>% arrange(year, month, day)
flights_db %>% mutate(speed = air_time / distance)
flights_db %>% summarise(delay = mean(dep_time))
```

The most important difference is that the expressions in `select()`, `filter()`, `arrange()`, `mutate()`, and `summarise()` are translated into SQL so they can be run on the database. The details of the translation are described in `vignette("sql-translation")`.

When working with databases, dplyr tries to be as lazy as possible:

* It never pulls data into R unless you explicitly ask for it.

* It delays doing any work until the last possible moment: it collects together
  everything you want to do and then sends it to the database in one step.

For example, take the following code:

```{r}
c1 <- filter(flights_db, year == 2013, month == 1, day == 1)
c2 <- select(c1, year, month, day, carrier, dep_delay, air_time, distance)
c3 <- mutate(c2, speed = distance / air_time * 60)
c4 <- arrange(c3, year, month, day, carrier)
```

Suprisingly, this sequence of operations never actually touches the database. It's not until you ask for the data (e.g. by printing `c4`) that dplyr generates the SQL and requests the results from the database. Even then it only pulls down 10 rows.

```{r}
c4
```

To pull down all the results use `collect()`, which returns a `tbl_df()`:

```{r}
collect(c4)
```

You can see the query dplyr will generate to pull down results by using `show_query()`

```{r}
c4 %>% show_query()
```

You can also ask the database how it plans to execute the query with `explain()`. The output for SQLite is described in more detail on the [SQLite website](http://www.sqlite.org/eqp.html). It's helpful if you're trying to figure out which indexes are being used.

dplyr tries to prevent you from accidentally performing expensive query operations:

* Because there's generally no way to determine how many rows a query will 
  return unless you actually run it, `nrow()` is always `NA`.

* Because you can't find the last few rows without executing the whole 
  query, you can't use `tail()`.

* Printing a remote tbl automatically uses `head()` so that the query
  only retrieves the first few rows.

```{r, error = TRUE}
nrow(flights_db)

tail(flights_db)

flights_db %>% head(6) %>% show_query()
```

## Grouping

SQLite lacks the window functions that are needed for grouped mutation and filtering. This means that the only really useful operations for grouped SQLite tables are found in `summarise()`. The grouped summarise from the introduction translates well - the only difference is that databases always drop NULLs (their equivalent of missing values), so we don't supply `na.rm = TRUE`.

```{r}
by_tailnum <- flights_db %>% group_by(tailnum)
delay <- by_tailnum %>% summarise(
  count = n(),
  dist = mean(distance),
  delay = mean(arr_delay)
)
delay <- delay %>% filter(count > 20, dist < 2000)
delay
```

Other databases do support window functions. You can learn about them in `vignette("window-functions")`. It's sometimes possible to simulate grouped filtering and mutation using self joins, which join the original table with a summarised version, but that topic is beyond the scope of this introduction.

## Other databases

Aside from SQLite, the overall workflow is essentially the same regardless of the database you're connecting to. The following sections go in to more details about the peculiarities of each database engine. All of these databases follow a client-server model - a computer that connects to the database and the computer that is running the database (the two may be one and the same but usually isn't). Getting one of these databases up and running is beyond the scope of this article, but there are plenty of tutorials available on the web.

### PostgreSQL

PostgreSQL is a considerably more powerful database than SQLite, and you can connect to it with `RPostgreSQL::PostgreSQL()`. You'll typically need to provide a `dbname`, `username`, `password`, `host`, and `port`.

PostgreQQL has:

* a much wider range of [built-in functions](http://www.postgresql.org/docs/9.3/static/functions.html)

* support for [window functions](http://www.postgresql.org/docs/9.3/static/tutorial-window.html), which allow grouped subset and mutates to work.

The following examples show how we can perform grouped filter and mutate operations with PostgreSQL. Because you can't filter on window functions directly, the SQL generated from the grouped filter is quite complex; so they instead have to go in a subquery.

```{r, eval = FALSE}
daily <- flights_postgres %>% group_by(year, month, day)

# Find the most and least delayed flight each day
bestworst <- daily %>% 
  select(flight, arr_delay) %>% 
  filter(arr_delay == min(arr_delay) || arr_delay == max(arr_delay))
bestworst %>% show_query()

# Rank each flight within a daily
ranked <- daily %>% 
  select(arr_delay) %>% 
  mutate(rank = rank(desc(arr_delay)))
ranked %>% show_query()
```

### MySQL and MariaDB

You can connect to MySQL and MariaDB (a recent fork of MySQL) using `[RMySQL](https://github.com/jeffreyhorner/RMySQL)::MySQL()`. You'll typically need to provide a `dbname`, `username`, `password`, `host`, and `port`.

In terms of functionality, MySQL lies somewhere between SQLite and PostgreSQL. It provides a wider range of [built-in functions](http://dev.mysql.com/doc/refman/5.0/en/functions.html), but it does not support window functions (so you can't do grouped mutates and filters).

### BigQuery

BigQuery is a hosted database server provided by Google. To connect, you need to provide your `project`, `dataset` and optionally a project for `billing` (if billing for `project` isn't enabled). After you create the src, your web browser will open and ask you to authenticate. Your credentials are stored in a local cache, so you should only need to do this once.

BigQuery supports only one SQL statement: [SELECT](https://developers.google.com/bigquery/query-reference). Fortunately this is all you need for data analysis. Within SELECT, BigQuery provides comprehensive coverage at a similar level to PostgreSQL.

## Picking a database

If you don't already have a database, here's some advice from my experiences setting up and running all of them. SQLite is by far the easiest to get started with, but the lack of window functions makes it limited for data analysis. PostgreSQL is not too much harder to use and has a wide range of built-in functions. Don't bother with MySQL/MariaDB: it's a pain to set up and the documentation is subpar. Google BigQuery might be a good fit if you have very large data, or if you're willing to pay (a small amount of) money to someone who'll look after your database.
